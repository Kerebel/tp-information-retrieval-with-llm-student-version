{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kerebel/tp-information-retrieval-with-llm-student-version/blob/main/1-Recherche%20d'information%20classique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awy3wXSGGKuK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1BGZgY-tvZ"
      },
      "source": [
        "### Import des bibliothèques logicielles et configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xrlW3KGKuM"
      },
      "source": [
        "Les lignes suivantes permettent d'instancier un objet la classe `IRSystem` représentant notre moteur de recherche et de charger les données en RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cstDrnVSGKuM",
        "outputId": "df7352cb-81a0-4d97-ddf1-1f413b6be82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tp-information-retrieval-with-llm-student-version'...\n",
            "remote: Enumerating objects: 2264, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 2264 (delta 31), reused 33 (delta 17), pack-reused 2212\u001b[K\n",
            "Receiving objects: 100% (2264/2264), 14.34 MiB | 20.26 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/tp-information-retrieval-with-llm-student-version\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Vérifie si le code est exécuté sur Google Colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Commandes à exécuter uniquement sur Google Colab\n",
        "    !git clone https://github.com/Kerebel/tp-information-retrieval-with-llm-student-version.git\n",
        "    %cd tp-information-retrieval-with-llm-student-version\n",
        "else:\n",
        "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
        "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2WKGecSGKuN"
      },
      "source": [
        "#### Chargement des données\n",
        "\n",
        "Les lignes ci-dessous permettent de charger les données qui sont un ensemble de 60 livres au format texte (.txt) d'[Henry Rider Haggard ](https://fr.wikipedia.org/wiki/Henry_Rider_Haggard).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZNWPECSs9wgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f8f0ad-b1b9-42bc-a1d5-1975cb71e5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading in documents...\n",
            "Stemming Documents...\n",
            "The Ivory Child 2841.txt\n",
            "    Doc 1 of 60: The Ivory Child\n",
            "Eric Brighteyes 2721.txt\n",
            "    Doc 2 of 60: Eric Brighteyes\n",
            "Red Eve 3094.txt\n",
            "    Doc 3 of 60: Red Eve\n",
            "Jess 5898.txt\n",
            "    Doc 4 of 60: Jess\n",
            "Stories by English Authors Africa (Selected by Scribners) 1980.txt\n",
            "    Doc 5 of 60: Stories by English Authors Africa (Selected by Scribners)\n",
            "Love Eternal 3709.txt\n",
            "    Doc 6 of 60: Love Eternal\n",
            "Child of Storm 1711.txt\n",
            "    Doc 7 of 60: Child of Storm\n",
            "Marie An Episode in The Life of the late Allan Quatermain 1690.txt\n",
            "    Doc 8 of 60: Marie An Episode in The Life of the late Allan Quatermain\n",
            "The Wizard 2893.txt\n",
            "    Doc 9 of 60: The Wizard\n",
            "Ayesha, the Return of She 5228.txt\n",
            "    Doc 10 of 60: Ayesha, the Return of She\n",
            "Lysbeth, a Tale of the Dutch 5754.txt\n",
            "    Doc 11 of 60: Lysbeth, a Tale of the Dutch\n",
            "The Wanderer's Necklace 3097.txt\n",
            "    Doc 12 of 60: The Wanderer's Necklace\n",
            "The Lady of Blossholme 3813.txt\n",
            "    Doc 13 of 60: The Lady of Blossholme\n",
            "Black Heart and White Heart 2842.txt\n",
            "    Doc 14 of 60: Black Heart and White Heart\n",
            "Allan and the Holy Flower 5174.txt\n",
            "    Doc 15 of 60: Allan and the Holy Flower\n",
            "She and Allan 5745.txt\n",
            "    Doc 16 of 60: She and Allan\n",
            "The Virgin of the Sun 3153.txt\n",
            "    Doc 17 of 60: The Virgin of the Sun\n",
            "Beatrice 3096.txt\n",
            "    Doc 18 of 60: Beatrice\n",
            "Elissa 2855.txt\n",
            "    Doc 19 of 60: Elissa\n",
            "Moon of Israel 2856.txt\n",
            "    Doc 20 of 60: Moon of Israel\n",
            "A Winter Pilgrimage (1901) 0600121.txt\n",
            "    Doc 21 of 60: A Winter Pilgrimage (1901)\n",
            "Morning Star 2722.txt\n",
            "    Doc 22 of 60: Morning Star\n",
            "Queen Sheba's Ring 2602.txt\n",
            "    Doc 23 of 60: Queen Sheba's Ring\n",
            "Fair Margaret 9780.txt\n",
            "    Doc 24 of 60: Fair Margaret\n",
            "Benita, an African romance 2761.txt\n",
            "    Doc 25 of 60: Benita, an African romance\n",
            "When the World Shook 0.txt\n",
            "    Doc 26 of 60: When the World Shook\n",
            "She 3155.txt\n",
            "    Doc 27 of 60: She\n",
            "The Witch's Head (1884) 0500791.txt\n",
            "    Doc 28 of 60: The Witch's Head (1884)\n",
            "Colonel Quaritch, V.C. A Tale of Country Life 11882.txt\n",
            "    Doc 29 of 60: Colonel Quaritch, V.C. A Tale of Country Life\n",
            "The Ghost Kings 8184.txt\n",
            "    Doc 30 of 60: The Ghost Kings\n",
            "Heu-Heu (1924) 0200191.txt\n",
            "    Doc 31 of 60: Heu-Heu (1924)\n",
            "Montezuma's Daughter 1848.txt\n",
            "    Doc 32 of 60: Montezuma's Daughter\n",
            "Allan Quatermain 711.txt\n",
            "    Doc 33 of 60: Allan Quatermain\n",
            "Hunter Quatermain's Story 2728.txt\n",
            "    Doc 34 of 60: Hunter Quatermain's Story\n",
            "A Yellow God an Idol of Africa 2857.txt\n",
            "    Doc 35 of 60: A Yellow God an Idol of Africa\n",
            "Regeneration 13434.txt\n",
            "    Doc 36 of 60: Regeneration\n",
            "Pearl-Maiden 5175.txt\n",
            "    Doc 37 of 60: Pearl-Maiden\n",
            "Swallow a tale of the great trek 4074.txt\n",
            "    Doc 38 of 60: Swallow a tale of the great trek\n",
            "Maiwa's Revenge 2713.txt\n",
            "    Doc 39 of 60: Maiwa's Revenge\n",
            "Doctor Therne 5764.txt\n",
            "    Doc 40 of 60: Doctor Therne\n",
            "Allan's Wife 2727.txt\n",
            "    Doc 41 of 60: Allan's Wife\n",
            "Long Odds 1918.txt\n",
            "    Doc 42 of 60: Long Odds\n",
            "Dawn 10892.txt\n",
            "    Doc 43 of 60: Dawn\n",
            "Cleopatra 2769.txt\n",
            "    Doc 44 of 60: Cleopatra\n",
            "The People of the Mist 6769.txt\n",
            "    Doc 45 of 60: The People of the Mist\n",
            "The Mahatma and the Hare 2764.txt\n",
            "    Doc 46 of 60: The Mahatma and the Hare\n",
            "Queen of the Dawn (1925) 0200381.txt\n",
            "    Doc 47 of 60: Queen of the Dawn (1925)\n",
            "Cetywayo and his White Neighbours 0.txt\n",
            "    Doc 48 of 60: Cetywayo and his White Neighbours\n",
            "Finished 1724.txt\n",
            "    Doc 49 of 60: Finished\n",
            "Stella Fregelius 6051.txt\n",
            "    Doc 50 of 60: Stella Fregelius\n",
            "King Solomon's Mines 2166.txt\n",
            "    Doc 51 of 60: King Solomon's Mines\n",
            "The Ancient Allan 5746.txt\n",
            "    Doc 52 of 60: The Ancient Allan\n",
            "Mr. Meeson's Will 11913.txt\n",
            "    Doc 53 of 60: Mr. Meeson's Will\n",
            "Wisdom's Daughter (1923) 0200181.txt\n",
            "    Doc 54 of 60: Wisdom's Daughter (1923)\n",
            "Nada the Lily 1207.txt\n",
            "    Doc 55 of 60: Nada the Lily\n",
            "The Brethren 2762.txt\n",
            "    Doc 56 of 60: The Brethren\n",
            "The World's Desire 2763.txt\n",
            "    Doc 57 of 60: The World's Desire\n",
            "The Tale of Three Lions 2729.txt\n",
            "    Doc 58 of 60: The Tale of Three Lions\n",
            "Smith and the Pharaohs, and other Tales 6073.txt\n",
            "    Doc 59 of 60: Smith and the Pharaohs, and other Tales\n",
            "Allan and the Ice Gods (1927) 0200201.txt\n",
            "    Doc 60 of 60: Allan and the Ice Gods (1927)\n"
          ]
        }
      ],
      "source": [
        "from classic_ir.IRSystem import *\n",
        "\n",
        "# !rm -rf ./data/RiderHaggard/stemmed\n",
        "ir_system = IRSystem()\n",
        "ir_system.read_data('./data/RiderHaggard') # chargement des données et prétraitement des documents (stemming)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZMLCILGKuN"
      },
      "source": [
        "### Exercice 1. - Construction de l'index inversé\n",
        "\n",
        "Ce premier exercice a pour objectif de construire l'index inversé non positionnel. L'attribut `self.inverted_index` est un tableau associatif qui associe une liste d'entiers (docIDs) à un mot (word).\n",
        "\n",
        "Documentation ici https://docs.python.org/3/library/collections.html#collections.defaultdict.\n",
        "\n",
        "Exercice : modifier la fonction `index` pour calculer l'index inversé.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Inverted Index Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iJrrS_wGKuN",
        "outputId": "7964221c-c261-4e26-a989-d98c52f1e0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing...\n",
            "===== Running tests =====\n",
            "Inverted Index Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 1. Indexation\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def index(self):\n",
        "    \"\"\"\n",
        "    Construit l'index inversé et le stocke dans self.inverted_index.\n",
        "    \"\"\"\n",
        "    print(\"Indexing...\")\n",
        "    self.tf = defaultdict(Counter) # tf est un dictionnaire qui à un document associe un Counter de mots.\n",
        "    inverted_index = defaultdict(list) # inverted_index est un dictionnaire qui à un mot associe une liste de documents.\n",
        "    i = 0\n",
        "    for doc in self.docs:\n",
        "      for word in doc:\n",
        "        if(i not in inverted_index[word]):\n",
        "          inverted_index[word].append(i)\n",
        "      i=i+1\n",
        "\n",
        "    self.inverted_index = inverted_index\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.index = index\n",
        "ir_system.index()\n",
        "run_tests(ir_system, part=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbxCw9mAGKuN"
      },
      "source": [
        "### Exercice 2. - Recherche booléenne\n",
        "\n",
        "Ce deuxième exercice a pour objectif d'implémenter la recherche booléenne. La requête `query` est une liste de mots _lemmatisés_ et l'algorithme doit rechercher les documents qui contiennent TOUS ces mots.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Boolean Retrieval Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIiAyZ0sGKuO",
        "outputId": "a1bdf72f-716d-4963-f553-f665539effd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Running tests =====\n",
            "Boolean Retrieval Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 2. Recherche booléenne\n",
        "def boolean_retrieve(self, query):\n",
        "    \"\"\"\n",
        "    A partir d'une requête sous la forme d'une liste de mots *lemmatisés*,\n",
        "    retourne la liste des documents dans lesquels *tous* ces mots apparaissent (ie une requête AND).\n",
        "    Retourne une liste vide si la requête ne retourne aucun document.\n",
        "\n",
        "    Dans le code ci-dessous, tous les documents répondent.\n",
        "    \"\"\"\n",
        "    relevant_docs = []\n",
        "    for i, doc in enumerate(self.docs):\n",
        "        contains_all_words = all(word in doc for word in query)\n",
        "        if contains_all_words:\n",
        "          # Ajoute indice du document à la liste\n",
        "            relevant_docs.append(i)\n",
        "\n",
        "    return relevant_docs\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.boolean_retrieve = boolean_retrieve\n",
        "run_tests(ir_system, part=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ce_Uz7GKuO"
      },
      "source": [
        "### Exercice 3. - Calcul des poids TF-IDF des termes dans les documents\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours. Utiliser le logarithme en base 10.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "Calculating tf-idf...\n",
        "===== Running tests =====\n",
        "TF-IDF Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfl3qztJGKuO",
        "outputId": "91b03d2e-ceb2-40ea-cec2-1c8b053f0299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating tf-idf...\n",
            "===== Running tests =====\n",
            "TF-IDF Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 3. calcul des scores tf-idf\n",
        "def compute_tfidf(self):\n",
        "    \"\"\"\n",
        "    Calcule les scores tf-idf pour tous les mots de tous les documents et les stocke dans self.tfidf.\n",
        "\n",
        "    Dans le code ci-dessous, les scores tf-idf sont tous nuls.\n",
        "    \"\"\"\n",
        "    print(\"Calculating tf-idf...\")\n",
        "\n",
        "    tf = defaultdict(Counter)\n",
        "    i = 0\n",
        "    for i in range(len(self.docs)):\n",
        "      for word in self.docs[i]:\n",
        "        tf[i][word] += 1\n",
        "\n",
        "    self.tfidf = defaultdict(Counter)\n",
        "    N = len(self.docs)  # N = nombre de documents\n",
        "    for word in self.vocab:\n",
        "      idf = math.log10(N / len(self.get_posting(word)))\n",
        "      for i in range(N):\n",
        "        try:\n",
        "          self.tfidf[i][word] = (1 + math.log10(tf[i][word])) * idf\n",
        "        except ValueError:\n",
        "          self.tfidf[i][word] = 0.\n",
        "\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.compute_tfidf = compute_tfidf\n",
        "ir_system.compute_tfidf()\n",
        "run_tests(ir_system, part=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkzudQubGKuO"
      },
      "source": [
        "### Exercice 4. - Calcul de la similarité cosinus.\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours en considérant les éléments suivants :\n",
        "- Ne considérer que la mesure TF pour calculer le poids des termes de la requête (on utilise pas IDF).\n",
        "- utiliser le logarithme en base 10.\n",
        "\n",
        "Exercice : modifier la fonction `rank_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Cosine Similarity Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3JDHKYdGKuO",
        "outputId": "bebd72b8-0a17-446b-da6c-4df24193ba64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Running tests =====\n",
            "Cosine Similarity Test\n",
            "    Score: 0 Feedback: 0/5 Correct. Accuracy: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 4. Similarité cosinus\n",
        "def rank_retrieve(self, query):\n",
        "    scores = [0.0 for _ in range(len(self.docs))]\n",
        "    query_tf = {}\n",
        "\n",
        "    # Calcul des term frequencies pour la requête\n",
        "    for term in query:\n",
        "        if term not in query_tf:\n",
        "            query_tf[term] = 0\n",
        "        query_tf[term] += 1\n",
        "\n",
        "    # Calcul des poids TF pour les documents et application de la similarité cosinus\n",
        "    for d in range(len(self.docs)):\n",
        "        doc_tf = {}\n",
        "        doc_length = len(self.docs[d])\n",
        "\n",
        "        for term in self.docs[d]:\n",
        "            if term not in doc_tf:\n",
        "                doc_tf[term] = 0\n",
        "            doc_tf[term] += 1\n",
        "\n",
        "        # Calcul du score de similarité cosinus\n",
        "        dot_product = 0.0\n",
        "        query_norm = 0.0\n",
        "        doc_norm = 0.0\n",
        "\n",
        "        for term, tf_query in query_tf.items():\n",
        "            tf_doc = doc_tf.get(term, 0)\n",
        "            dot_product += tf_query * tf_doc  # Produit scalaire\n",
        "\n",
        "            query_norm += tf_query ** 2  # Norme du vecteur requête\n",
        "            doc_norm += tf_doc ** 2  # Norme du vecteur document\n",
        "\n",
        "        query_norm = math.sqrt(query_norm)\n",
        "        doc_norm = math.sqrt(doc_norm)\n",
        "\n",
        "        # Éviter la division par zéro\n",
        "        if query_norm != 0 and doc_norm != 0:\n",
        "            scores[d] = dot_product / (query_norm * doc_norm)\n",
        "\n",
        "    # Tri des scores\n",
        "    ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
        "                                        key=lambda pair: pair[1], reverse=True)]\n",
        "    results = []\n",
        "    for i in range(len(ranking)):  # Retourner tous les résultats sans limite\n",
        "        results.append((ranking[i], scores[ranking[i]]))\n",
        "    return results\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.rank_retrieve = rank_retrieve\n",
        "run_tests(ir_system, part=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}